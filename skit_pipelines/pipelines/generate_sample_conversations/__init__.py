import kfp
from kfp.components import OutputPath
from typing import List

from skit_pipelines import constants as pipeline_constants
from skit_pipelines.components import (
    sample_conversations_generator_op,
    upload2s3_op,
    zip_file_and_notify_op,
    slack_notification_op
)


@kfp.dsl.pipeline(
    name="Generate sample conversations",
    description="Generate sample conversations based on the situation data provided",
)
def generate_sample_conversations(
    *,
    scenarios: List[str],
    prompt: str = "",
    output_dir: str = "",
    filename: str = "",
    llm_trainer_repo_name: str = "LLMtrainer",
    llm_trainer_repo_branch: str = "main",
    model: str = 'gpt-4',
    n_iter: int = 1,
    n_choice: int = 2,
    temperature: float = 0.99,
    notify: str = "",
    channel: str = "",
    slack_thread: str = "",
    storage_options: str = '{"type": "s3","bucket": "kubeflow-skit"}'
    ):
    """
    A pipeline to sample conversations given a situation
    
    .. generato_sample_conversations:

    Example payload to invoke via slack integrations:

    A minimal example:

        @charon run generate_sample_conversations

        .. code-block:: python

            {
                "scenarios": ["The user wants to talk to a human agent, so the agent transfers the call"],
                "llm_trainer_repo_name": "LLMtrainer",
                "llm_trainer_repo_branch": "main"
                }


    A full available parameters example:

        @charon run generate_sample_conversations

        .. code-block:: python

            {
                "scenarios": ["The user wants to talk to a human agent, so the agent transfers the call"],
                "llm_trainer_repo_name": "LLMtrainer",
                "llm_trainer_repo_branch": "main",
            }

    :param scenarios: The scenarios for generating the conversations
    :type scenarios: optional
    
    :param prompt: Prompt to the model for data generation
    type prompt: str
    
    :param output_dir: The output directory where the generated conversations gets stored
    :type output_dir: str

    :param filename: Acts as a prfix to the default naming used
    :type filename: str

    :param llm_trainer_repo_name: The conversation generation repo name in Github.
    :type llm_trainer_repo_name: str
    
    :param llm_trainer_repo_branch: The branch name in the conversation generation repo to use , defaults to main.
    :type llm_trainer_repo_branch: str, optional
    
    :param model: Optional model to be used for generating data 
    :type model: str
    
    :param n_iter: No of times we make iterate on scenarios list to generate conversations
    type n_iter: int
    
    :param n_choice: No of convs generated in a single time from a scenario.
    type n_choice: int
    
    :param temperature: Temperature
    type temperature: float
    
    :param notify: Whether to send a slack notification, defaults to ""
    :type notify: str, optional

    :param channel: The slack channel to send the notification, defaults to ""
    :type channel: str, optional

    :param slack_thread: The slack thread to send the notification, defaults to ""
    :type slack_thread: str, optional

    """

    prompt_generation = sample_conversations_generator_op(
        scenarios=scenarios,
        llm_trainer_repo_name=llm_trainer_repo_name,
        llm_trainer_repo_branch=llm_trainer_repo_branch,
        output_dir=output_dir,
        filename=filename,
        model=model,
        prompt=prompt,
        n_iter=n_iter,
        n_choice=n_choice,
        temperature=temperature
    )

    prompt_s3_upload = upload2s3_op(
            path_on_disk=prompt_generation.outputs["output"],
            reference = 'pipeline_uploads/generated_conversations/',
            storage_options=storage_options,
            upload_as_directory=True,
            ext=""
        )

    with kfp.dsl.Condition(notify != "", "notify").after(prompt_s3_upload):
        notification_text_1 = f"Generated conversations are successfully uploaded to s3."
        code_block = f"aws s3 cp {prompt_s3_upload.output} ."
        prompt_s3_notif = slack_notification_op(
            message=notification_text_1,
            channel=channel,
            cc=notify,
            code_block=code_block,
            thread_id=slack_thread,
        )
        
        prompt_s3_notif.execution_options.caching_strategy.max_cache_staleness = (
            "P0D"  # disables caching
        )
        
        notification_text_2 = "Here is the ZIP file generated by the Generate Sample conversations Pipeline."
        zip_file_and_notify = zip_file_and_notify_op(
                    path_on_disk = prompt_generation.outputs["output"], 
                    message = notification_text_2,
                    channel = channel,
                    thread_id = slack_thread,
                    file_title = 'generated_conversations',
                    file_name = 'generated_conversations.zip',
                    ).after(prompt_s3_notif)
        
        zip_file_and_notify.execution_options.caching_strategy.max_cache_staleness = (
            "P0D"  # disables caching
        )


__all__ = ["generate_sample_conversations"]
